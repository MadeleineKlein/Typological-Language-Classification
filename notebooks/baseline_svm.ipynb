{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Character TF-IDF + Linear SVM\n",
        "\n",
        "This notebook provides a reproducible experiment for the initial typological language identification baseline. It uses the bundled sample dataset so we can sanity-check the pipeline end-to-end before scaling to WiLI-2018 subsets.\n",
        "\n",
        "**What this notebook covers**\n",
        "- Load the curated sample dataset\n",
        "- Split into training and evaluation folds\n",
        "- Vectorize with character n-grams (TF-IDF)\n",
        "- Fit a linear SVM classifier (`LinearSVC`)\n",
        "- Report accuracy, macro metrics, and a confusion matrix heatmap\n",
        "\n",
        "Once this flow looks healthy, you can point the same logic at WiLI subsets generated via `python -m src.data.wili_downloader prepare-subset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from src.config.settings import SAMPLE_DATA, TrainingConfig\n",
        "from src.features.text_vectorizer import build_char_vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = TrainingConfig()\n",
        "df = pd.read_csv(SAMPLE_DATA)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"],\n",
        "    df[\"language\"],\n",
        "    test_size=config.test_size,\n",
        "    random_state=config.random_state,\n",
        "    stratify=df[\"language\"],\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        (\n",
        "            \"vectorizer\",\n",
        "            build_char_vectorizer(\n",
        "                ngram_range=config.ngram_range,\n",
        "                max_features=config.max_features,\n",
        "                use_idf=config.use_idf,\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"classifier\",\n",
        "            LinearSVC(\n",
        "                C=config.c_value,\n",
        "                class_weight=config.class_weight,\n",
        "                random_state=config.random_state,\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "report = classification_report(y_test, y_pred, output_dict=False)\n",
        "print(report)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=sorted(df[\"language\"].unique()))\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=sorted(df[\"language\"].unique()),\n",
        "    yticklabels=sorted(df[\"language\"].unique()),\n",
        ")\n",
        "plt.title(\"Confusion Matrix - Sample Dataset\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
